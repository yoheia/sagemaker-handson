{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer の学習と推論を SageMaker で行う\n",
    "\n",
    "MNISTデータセットを対象にSageMakerと**独自のChainerのコード**を利用してMLPの学習と推論を行います。学習の方法としては、高性能なトレーニングインスタンスを1つまたは複数立ち上げて学習を行います。\n",
    "\n",
    "\n",
    "## 目次\n",
    "1. [準備](#準備)\n",
    "2. [データの取得とS3へのアップロード](#データの取得とS3へのアップロード)\n",
    "3. [学習スクリプトの確認](#学習スクリプトの確認)\n",
    "4. [モデルの学習](#モデルの学習)\n",
    "5. [モデルの推論を実行](#モデルの推論を実行)\n",
    "6. [エンドポイントの削除](#エンドポイントの削除)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得とS3へのアップロード\n",
    "\n",
    "ここでは、`Chainer` でサポートされている関数を使って、MNIST データをダウンロードします。SageMaker の学習時に利用するデータは、S3 に置く必要があります。ここでは、ローカルにダウンロードした MNIST データを npz 形式で固めてから、SageMaker のラッパー関数を使って S3 にアップロードします。S3に上げる際のファイルの形式は、学習スクリプト内での読み込み処理と一致している限り指定はありません。\n",
    "\n",
    "デフォルトでは SageMaker は `sagemaker-{region}-{your aws account number}` というバケットを使用します。当該バケットがない場合には、自動で新しく作成します。`upload_data()` メソッドの引数に bucket=XXXX という形でデータを配置するバケットを指定することも可能です。\n",
    "\n",
    "![図1](./images/image1.jpg \"図1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Download MNIST dataset\n",
    "print('Downloading Dataset...')\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "\n",
    "# Extract data and labels from dataset \n",
    "train_images = np.array([data[0] for data in train])\n",
    "train_labels = np.array([data[1] for data in train])\n",
    "test_images = np.array([data[0] for data in test])\n",
    "test_labels = np.array([data[1] for data in test])\n",
    "\n",
    "# Save the data and labels as .npz into local directories and upload them to S3\n",
    "print('Uploading Dataset to S3...')\n",
    "try:\n",
    "    os.makedirs('/tmp/data/train')\n",
    "    os.makedirs('/tmp/data/test')\n",
    "\n",
    "    np.savez('/tmp/data/train/train.npz', images=train_images, labels=train_labels)\n",
    "    np.savez('/tmp/data/test/test.npz', images=test_images, labels=test_labels)\n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path=os.path.join('/tmp/data', 'train'),\n",
    "        key_prefix='notebook/chainer/mnist')\n",
    "    test_input = sagemaker_session.upload_data(\n",
    "        path=os.path.join('/tmp/data', 'test'),\n",
    "        key_prefix='notebook/chainer/mnist')\n",
    "finally:\n",
    "    shutil.rmtree('/tmp/data')\n",
    "print('Finished')\n",
    "print('train_input: {}'.format(train_input))\n",
    "print('test_input: {}'.format(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習スクリプトの確認\n",
    "\n",
    "SageMakerで、Chainer、Tensorflowなどのフレームワークを利用して深層学習を行うためには、このnotebook以外に**学習スクリプトを作成する必要があります**。学習スクリプトとはモデルや学習方法を記述した.pyファイルで、このnotebookには`chainer_mnist.py`という学習スクリプトを同じフォルダに用意しています。ノートブックインスタンスでfit関数を呼び出すと、entry_pointに指定したスクリプトを起点に学習が行われます。\n",
    "\n",
    "chainerを利用する場合は、学習スクリプトの`__main__`関数内にモデルの記述や学習方法を記載すればよく、SageMakerを使う以前のChainerのコードを概ねそのまま利用することができます。また、環境変数経由で入力データの場所や GPU の数などを取得することが可能です。これは `argparse` 経由で `main` 関数内で受け取ることができます。詳細は[こちら](https://sagemaker.readthedocs.io/en/stable/using_chainer.html#preparing-the-chainer-training-script)をご覧ください。\n",
    "\n",
    "また推論時の処理は、`model_fn` で学習済みモデルをロードする部分だけ記述する必要があります。その他オプションで、前処理、推論処理、後処理部分を `input_fn`、 `predict_fn`、 `output_fn` で書くこともできます。デフォルトでは、`application/x-npy` コンテントタイプで指定される、NumPy 配列を入力として受け取ります。 詳細は[こちら](https://sagemaker.readthedocs.io/en/stable/using_chainer.html#the-sagemaker-chainer-model-server)をご覧ください。\n",
    "\n",
    "以下のセルを実行して学習スクリプトの中身を表示してみます。すると、`class MLP(chainer.Chain)`といったモデルの定義、`__main__`の中に学習のコードが書かれていることがわかります。また、chainerMNのoptimizerを使用して分散学習を行うようになっています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pygmentize 'chainer_mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習\n",
    "\n",
    "`Estimator` クラスの子クラスの `Chainer` オブジェクトを作成し、`fit()` メソッドで学習ジョブを実行します。 `entry_point` で指定したローカルのスクリプトが、学習用のコンテナ内で実行されます。また合わせて `source_dir` でローカルのディレクトリを指定することで、依存するスクリプト群をコンテナにコピーして、学習時に使用することが可能です。`source_dir`の中に`requirements.txt`ファイルを含めることで、学習時にpipで入るパッケージを利用することができます。\n",
    "\n",
    "単一ノードで学習したい場合は、`train_instance_count=1`として、学習用インスタンスを`instance_type`に指定します。複数ノードによる学習は、`train_instance_count`を1より大きくすることで実行できます。複数ノードの場合には、分散学習となるようにエントリーポイントにChainerMNを利用した実装が必要になります。あとで学習の結果を参照するためにジョブの名前を記録しておきます。\n",
    "\n",
    "学習が始まると、学習インスタンスから出力されたログがノートブックインスタンスに表示されます。`fit()`メソッドの引数に`wait=False`を指定することで、非同期に学習ジョブを実行することも可能です。\n",
    "\n",
    "![図2](./images/image2.jpg \"図2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='chainer_mnist.py', role=role,\n",
    "                            train_instance_count=1, train_instance_type=instance_type,\n",
    "                            hyperparameters={'epochs': 3, 'batch_size': 128})\n",
    "\n",
    "chainer_estimator.fit({'train': train_input, 'test': test_input})\n",
    "\n",
    "# Keep the job name for checking training loss later \n",
    "training_job = chainer_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの推論を実行\n",
    "\n",
    "\n",
    "推論を行うために学習したモデルをデプロイします。ここでは、学習用インスタンスで学習した結果からデプロイしましょう。`deploy()` メソッドでは、デプロイ先エンドポイントのインスタンス数、インスタンスタイプを指定します。\n",
    "\n",
    "![図3](./images/image3.jpg \"図3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デプロイが終わったら実際に手書き文字認識を行ってみましょう。ランダムに5枚選んで推論をしてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "indices = random.sample(range(test_images.shape[0] - 1), num_samples)\n",
    "images, labels = test_images[indices], test_labels[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "prediction = predictor.predict(images)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('The predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "全て終わったら，エンドポイントを削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
