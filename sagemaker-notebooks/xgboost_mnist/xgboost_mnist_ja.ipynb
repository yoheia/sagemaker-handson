{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker XGBoost アルゴリズムによる多クラス分類\n",
    "---\n",
    "## 目次\n",
    "\n",
    "1. [はじめに](#はじめに)\n",
    "2. [データの準備](#データの準備)\n",
    "  1. [データのダウンロード](#データのダウンロード)\n",
    "  2. [データの加工とS3へのアップロード](#データの加工とS3へのアップロード)\n",
    "3. [XGBoostモデルの学習](#XGBoostモデルの学習)\n",
    "4. [推論](#推論)\n",
    "  1. [XGBoostをインストールして推論](#XGBoostをインストールして推論)\n",
    "  2. [エンドポイントを作成して推論](#エンドポイントを作成して推論)\n",
    "---\n",
    "## はじめに\n",
    "\n",
    "\n",
    "このnotebookでは、[deeplearning.net](http://deeplearning.net/)で公開されているMNIST (Modified National Institute of Standards and Technology)の手書き数字データセットを利用して、書かれている数字を認識します。MNISTのサンプルを示します。\n",
    "![MNISTサンプル](./images/mnist.png \"サンプル\")\n",
    "\n",
    "MNISTには、学習用に60,000枚、テスト用に10,000枚のラベル付き画像が用意されており、画像の解像度は28x28です。各画像にどの数字が書かれているかを判別するために、多クラス分類器として有用なXGBoostを利用します。**XGBoostはBuilt-inアルゴリズムとして、SageMakerに元々組み込まれていますので、学習用データをご用意頂ければ簡単に機械学習を行うことができます。** その他のBuilt-inアルゴリズムに関しては、[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/algos.html)をご覧ください。\n",
    "\n",
    "\n",
    "---\n",
    "## データの準備\n",
    "\n",
    "### データのダウンロード\n",
    "まず[deeplearning.net](http://deeplearning.net/)からデータをダウンロードします。ダウンロードされるファイルはバイナリで、ファイルを読み込むと`train_set, valid_set, test_set`の3つを得ることができます。各セットには、**画像を表す配列**と**ラベルを表す配列をリスト**として含んでいます。画像枚数は、それぞれ50,000枚、10,000枚、10,000枚です。したがって、`train_set`には50,000x784の画像を表す配列と、50,000のラベルを表す配列が入っていることになります。ここで784とは28x28の画像をベクトル化したときの次元数を表します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "print('Loading dataset...')\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの加工とS3へのアップロード\n",
    "\n",
    "データをダウンロードしたら、SageMaker XGBoostが読み込めるファイル形式に加工します。SageMaker XGBoostが読み込めるファイルはcsvまたはlibsvmです。ここではlibsvmの形式に加工します。libsvm形式とは以下のような形式です。\n",
    "```\n",
    "3 1:0.0 2:0.0 3:0.0 4:0.0 5:0.0 6:0.0 7:0.0 8:0.0 9:0.0 10:0.0 11:0.0 12:0.0 13:0.0 ...\n",
    "```\n",
    "最初の数字が`ラベル`を表し、以降は`特徴のindex:特徴量`が並びます。以下のセルでは`to_libsvm`の関数がlibsvm形式への変換を行います。\n",
    "\n",
    "以降、高性能な学習用インスタンスを立ち上げてモデルを学習するため、それらのインスタンスがアクセスできるようにファイルをS3に置く必要があります。\n",
    "各セットを`data.train, data.valid, data.test`というファイル名で保存したら、 `sess.upload_data`を利用してS3にアップロードします。S3のアップロード先は、バケット名が`default_backet()`によって自動設定される`sagemaker-{region}-{AWS account ID}`で、prefixが`notebook/xgboost/mnist`となります。バケット名も自由に設定できますが、世界中で唯一の名前となるような設定が必要です。\n",
    "\n",
    "![図1](./images/image1.jpg \"図1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "import pickle\n",
    "import gzip\n",
    "import sagemaker\n",
    "\n",
    "print('Processing Data...')\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'notebook/xgboost/mnist'\n",
    "def to_libsvm(f_name, labels, values):\n",
    "    with open(f_name,'w',encoding = 'utf-8') as f:\n",
    "        content = '\\n'.join(['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)])\n",
    "        f.write(content)        \n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "\n",
    "to_libsvm('data.train', train_set[1], train_set[0])\n",
    "to_libsvm('data.valid', valid_set[1], valid_set[0])\n",
    "to_libsvm('data.test', test_set[1], test_set[0])\n",
    "\n",
    "print('Processing Data...')\n",
    "\n",
    "train_input = sess.upload_data(\n",
    "        path='data.train', \n",
    "        key_prefix=prefix)\n",
    "valid_input = sess.upload_data(\n",
    "        path='data.valid', \n",
    "        key_prefix=prefix)\n",
    "test_input = sess.upload_data(\n",
    "        path='data.test', \n",
    "        key_prefix=prefix)\n",
    "\n",
    "print('Finished')\n",
    "print('train_input: {}'.format(train_input))\n",
    "print('valid_input: {}'.format(valid_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoostモデルの学習\n",
    "\n",
    "ファイルをS3に置くことができたら学習を始めます。まず学習をするためのコンテナのイメージを指定する必要があります。各アルゴリズムに対応するイメージのリストは以下にあります。https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "学習を行うまでの流れは以下の通りです。\n",
    "1. sagemakerのEstimatorを指定する。学習に使用するインスタンスとその数、モデルを出力するS3のフォルダを指定します。\n",
    "1. ハイパーパラメータを指定します。ここでは多クラス分類なので、`objective='multi:softmax',num_class=10`の指定が必要です。\n",
    "1. S3のファイルパスを与えてfitします。\n",
    "\n",
    "\n",
    "![図2](./images/image2.jpg \"図2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri('ap-northeast-1', 'xgboost')\n",
    "\n",
    "training_job_name = 'DEMO-xgboost-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                   sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(eta=0.1,\n",
    "                        objective='multi:softmax',\n",
    "                        num_class=10,\n",
    "                        num_round=25)\n",
    "\n",
    "xgb.fit({'train': train_input, 'validation': valid_input}, job_name = training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n",
    "\n",
    "推論を行う方法として主に以下の２種類の方法があります。\n",
    "- **ノートブックインスタンスにxgboostをインストールしてローカルで推論する**  \n",
    "ノートブックインスタンスで推論を行う場合、推論用のインスタンスを立ち上げる必要がありません。開発段階において様々なモデルを試したいときに、推論用のインスタンスを立ち上げることなく、結果を知りたい場合に便利です。\n",
    "\n",
    "- **エンドポイントを作成して推論する**  \n",
    "サービスとして推論を利用するとき、サービスの負荷などを考慮して、ノートブックインスタンスよりも高性能なインスタンスで推論したい場合があります。その場合は、エンドポイントを作成して推論するほうが良いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostをインストールして推論\n",
    "\n",
    "まずXGBoostをcondaからインストールします。インストールが終わると、SageMaker XGBoostで学習したモデルS3からダウンロードしてXGBoostに読み込ませます。これで予測をする準備が整いました。\n",
    "\n",
    "ローカルにある`data.test`をXGBoostが扱う`DMatrix`形式にして読み込み、`predict`するとテストデータに対する予測結果を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブックインスタンスにxgboostをインストール\n",
    "try:\n",
    "    import xgboost as xg\n",
    "    print('XGboost {} has already been installed. '.format(xg.__version__))\n",
    "except:\n",
    "    !source activate python3 && conda install -y  -c conda-forge xgboost\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    import xgboost as xg\n",
    "    print('XGboost {} is installed. '.format(xg.__version__))\n",
    "    \n",
    "    \n",
    "# S3にあるファイルをダウンロードして解凍\n",
    "import boto3\n",
    "import botocore\n",
    "s3 = boto3.resource('s3')\n",
    "model_location =prefix +'/output/' + training_job_name + '/output/model.tar.gz'\n",
    "print(\"The model is saved at {}\".format('s3://' + bucket +'/'+model_location))\n",
    "try:\n",
    "    s3.Bucket(bucket).download_file(model_location, 'model.tar.gz')\n",
    "    !tar -zxvf model.tar.gz\n",
    "    print('Downloading and extracting the model are done.')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "        import sys\n",
    "        sys.exit()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# モデルのロード\n",
    "import pickle as pkl\n",
    "xgb_model = pkl.load(open('xgboost-model','rb'))\n",
    "print('Loading model is done.')\n",
    "\n",
    "#テストデータを読み込んで手書き文字(0-9)を分類\n",
    "dtest = xg.DMatrix('data.test')\n",
    "prediction = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測結果の確認\n",
    "\n",
    "以下ではランダムに1枚の画像を選んで、その画像に対する識別結果と画像を表示します。選ぶ画像を変えたい場合は、以下のセルを再実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "select_row = np.random.choice(range(len(test_set[1])))\n",
    "feature = test_set[0][select_row]\n",
    "pred_result = int(prediction[select_row])\n",
    "plt.imshow(feature.reshape(28, 28), cmap=cm.gray_r)\n",
    "print('Prediction: {}'.format(pred_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントを作成して推論\n",
    "**ハンズオンではオプションとします。エンドポイント作成については以降でも実施します。  \n",
    "もしエンドポイントを作成した場合は課金が発生しますので、後述の方法でエンドポイントの削除を忘れないようにしましょう。**\n",
    "\n",
    "### エンドポイントの作成\n",
    "SageMakerを用いると、インスタンス数とタイプを指定して`deploy`するだけでエンドポイントを作成できます。\n",
    "\n",
    "![図3](./images/image3.jpg \"図3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンドポイントによる推論\n",
    "\n",
    "このデモでは、ノートブックインスタンスから推論APIを叩いてみます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#エンドポイントが受け取るcontentを指定します。\n",
    "from sagemaker.predictor import csv_serializer\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "#ランダムにテストデータを選んで推論します。\n",
    "#予測の部分はpredictの部分だけです。\n",
    "select_row = np.random.choice(range(len(test_set[1])))\n",
    "feature = test_set[0][select_row]\n",
    "pred_result = xgb_predictor.predict(test_set[0][select_row])\n",
    "print(pred_result)\n",
    "pred_result = int(float(pred_result.decode(\"utf-8\")))\n",
    "plt.imshow(feature.reshape(28, 28), cmap=cm.gray_r)\n",
    "print('Prediction: {}'.format(pred_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
